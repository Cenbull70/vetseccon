{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sqlalchemy\n",
    "from datetime import timedelta, datetime\n",
    "import time\n",
    "import os\n",
    "import ipaddress\n",
    "from collections import defaultdict\n",
    "from decimal import Decimal\n",
    "from collections import Counter\n",
    "import geoip2.database\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import List, Dict, Optional, Any, Union\n",
    "from ipaddress import IPv4Address\n",
    "from uuid import uuid4\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>Malicious_IP</th>\n",
       "      <th>customers</th>\n",
       "      <th>mitigation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-08-27 08:37:01</td>\n",
       "      <td>139.87.112.128</td>\n",
       "      <td>Customer_63</td>\n",
       "      <td>mitigation_2890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-14 14:16:51</td>\n",
       "      <td>114.43.150.211</td>\n",
       "      <td>Customer_10</td>\n",
       "      <td>mitigation_1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-07-14 14:16:33</td>\n",
       "      <td>117.158.103.107</td>\n",
       "      <td>Customer_10</td>\n",
       "      <td>mitigation_1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-07-14 14:14:02</td>\n",
       "      <td>207.90.244.20</td>\n",
       "      <td>Customer_10</td>\n",
       "      <td>mitigation_1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-07-14 14:17:24</td>\n",
       "      <td>207.90.244.25</td>\n",
       "      <td>Customer_10</td>\n",
       "      <td>mitigation_1438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 date     Malicious_IP    customers  \\\n",
       "0           0  2025-08-27 08:37:01   139.87.112.128  Customer_63   \n",
       "1           1  2025-07-14 14:16:51   114.43.150.211  Customer_10   \n",
       "2           2  2025-07-14 14:16:33  117.158.103.107  Customer_10   \n",
       "3           3  2025-07-14 14:14:02    207.90.244.20  Customer_10   \n",
       "4           4  2025-07-14 14:17:24    207.90.244.25  Customer_10   \n",
       "\n",
       "        mitigation  \n",
       "0  mitigation_2890  \n",
       "1  mitigation_1438  \n",
       "2  mitigation_1438  \n",
       "3  mitigation_1438  \n",
       "4  mitigation_1438  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_ips = pd.read_csv(\"Malicious_ips.csv\")\n",
    "malicious_ips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>customers</th>\n",
       "      <th>mitigation</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>duration</th>\n",
       "      <th>industry</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>bps_average</th>\n",
       "      <th>bps_pct95</th>\n",
       "      <th>bps_max</th>\n",
       "      <th>pps_average</th>\n",
       "      <th>pps_pct95</th>\n",
       "      <th>pps_max</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Customer_1</td>\n",
       "      <td>mitigation_1</td>\n",
       "      <td>2025-06-16 09:19:35</td>\n",
       "      <td>2025-06-16 09:35:44</td>\n",
       "      <td>968</td>\n",
       "      <td>Communication Service Providers</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>Middle East</td>\n",
       "      <td>1.328188e+09</td>\n",
       "      <td>1.763629e+09</td>\n",
       "      <td>1.827236e+09</td>\n",
       "      <td>168520.0</td>\n",
       "      <td>218571.0</td>\n",
       "      <td>227094.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Customer_2</td>\n",
       "      <td>mitigation_2</td>\n",
       "      <td>2025-06-01 00:13:41</td>\n",
       "      <td>2025-06-01 00:23:11</td>\n",
       "      <td>570</td>\n",
       "      <td>Communication Service Providers</td>\n",
       "      <td>USA</td>\n",
       "      <td>North America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Total Traffic,UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Customer_3</td>\n",
       "      <td>mitigation_3</td>\n",
       "      <td>2025-06-01 00:34:25</td>\n",
       "      <td>2025-06-05 20:37:02</td>\n",
       "      <td>417757</td>\n",
       "      <td>Communication Service Providers</td>\n",
       "      <td>USA</td>\n",
       "      <td>North America</td>\n",
       "      <td>2.109800e+04</td>\n",
       "      <td>1.062880e+05</td>\n",
       "      <td>1.660320e+05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Customer_3</td>\n",
       "      <td>mitigation_4</td>\n",
       "      <td>2025-06-01 02:26:59</td>\n",
       "      <td>2025-06-01 10:24:04</td>\n",
       "      <td>28625</td>\n",
       "      <td>Communication Service Providers</td>\n",
       "      <td>USA</td>\n",
       "      <td>North America</td>\n",
       "      <td>3.730193e+06</td>\n",
       "      <td>1.253784e+07</td>\n",
       "      <td>8.284634e+07</td>\n",
       "      <td>6218.0</td>\n",
       "      <td>23510.0</td>\n",
       "      <td>27109.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Customer_4</td>\n",
       "      <td>mitigation_5</td>\n",
       "      <td>2025-06-01 07:13:59</td>\n",
       "      <td>2025-06-01 07:18:11</td>\n",
       "      <td>252</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Middle East</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TCP ACK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   customers    mitigation                start  \\\n",
       "0           0  Customer_1  mitigation_1  2025-06-16 09:19:35   \n",
       "1           1  Customer_2  mitigation_2  2025-06-01 00:13:41   \n",
       "2           2  Customer_3  mitigation_3  2025-06-01 00:34:25   \n",
       "3           3  Customer_3  mitigation_4  2025-06-01 02:26:59   \n",
       "4           4  Customer_4  mitigation_5  2025-06-01 07:13:59   \n",
       "\n",
       "                  stop  duration                         industry  \\\n",
       "0  2025-06-16 09:35:44       968  Communication Service Providers   \n",
       "1  2025-06-01 00:23:11       570  Communication Service Providers   \n",
       "2  2025-06-05 20:37:02    417757  Communication Service Providers   \n",
       "3  2025-06-01 10:24:04     28625  Communication Service Providers   \n",
       "4  2025-06-01 07:18:11       252               Financial Services   \n",
       "\n",
       "        country         region   bps_average     bps_pct95       bps_max  \\\n",
       "0          Iraq    Middle East  1.328188e+09  1.763629e+09  1.827236e+09   \n",
       "1           USA  North America           NaN           NaN           NaN   \n",
       "2           USA  North America  2.109800e+04  1.062880e+05  1.660320e+05   \n",
       "3           USA  North America  3.730193e+06  1.253784e+07  8.284634e+07   \n",
       "4  Saudi Arabia    Middle East           NaN           NaN           NaN   \n",
       "\n",
       "   pps_average  pps_pct95   pps_max            vectors  \n",
       "0     168520.0   218571.0  227094.0                NaN  \n",
       "1          NaN        NaN       NaN  Total Traffic,UDP  \n",
       "2          5.0       39.0      75.0                NaN  \n",
       "3       6218.0    23510.0   27109.0                NaN  \n",
       "4          NaN        NaN       NaN            TCP ACK  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_data = pd.read_csv(\"attack_data.csv\")\n",
    "attack_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(malicious_ips, attack_data, on='mitigation', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Malicious_IP</th>\n",
       "      <th>customer</th>\n",
       "      <th>mitigation</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>duration</th>\n",
       "      <th>industry</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>bps_average</th>\n",
       "      <th>bps_pct95</th>\n",
       "      <th>bps_max</th>\n",
       "      <th>pps_average</th>\n",
       "      <th>pps_pct95</th>\n",
       "      <th>pps_max</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-27 08:37:01</td>\n",
       "      <td>139.87.112.128</td>\n",
       "      <td>Customer_63</td>\n",
       "      <td>mitigation_2890</td>\n",
       "      <td>2025-08-27 08:36:29</td>\n",
       "      <td>2025-08-27 08:41:23</td>\n",
       "      <td>293</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>France</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Total Traffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-14 14:16:51</td>\n",
       "      <td>114.43.150.211</td>\n",
       "      <td>Customer_10</td>\n",
       "      <td>mitigation_1438</td>\n",
       "      <td>2025-07-14 14:09:57</td>\n",
       "      <td>2025-07-14 14:21:54</td>\n",
       "      <td>716</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Total Traffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-14 14:16:33</td>\n",
       "      <td>117.158.103.107</td>\n",
       "      <td>Customer_10</td>\n",
       "      <td>mitigation_1438</td>\n",
       "      <td>2025-07-14 14:09:57</td>\n",
       "      <td>2025-07-14 14:21:54</td>\n",
       "      <td>716</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Total Traffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-14 14:14:02</td>\n",
       "      <td>207.90.244.20</td>\n",
       "      <td>Customer_10</td>\n",
       "      <td>mitigation_1438</td>\n",
       "      <td>2025-07-14 14:09:57</td>\n",
       "      <td>2025-07-14 14:21:54</td>\n",
       "      <td>716</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Total Traffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-14 14:17:24</td>\n",
       "      <td>207.90.244.25</td>\n",
       "      <td>Customer_10</td>\n",
       "      <td>mitigation_1438</td>\n",
       "      <td>2025-07-14 14:09:57</td>\n",
       "      <td>2025-07-14 14:21:54</td>\n",
       "      <td>716</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Total Traffic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date     Malicious_IP     customer       mitigation  \\\n",
       "0 2025-08-27 08:37:01   139.87.112.128  Customer_63  mitigation_2890   \n",
       "1 2025-07-14 14:16:51   114.43.150.211  Customer_10  mitigation_1438   \n",
       "2 2025-07-14 14:16:33  117.158.103.107  Customer_10  mitigation_1438   \n",
       "3 2025-07-14 14:14:02    207.90.244.20  Customer_10  mitigation_1438   \n",
       "4 2025-07-14 14:17:24    207.90.244.25  Customer_10  mitigation_1438   \n",
       "\n",
       "                 start                 stop  duration            industry  \\\n",
       "0  2025-08-27 08:36:29  2025-08-27 08:41:23       293  Financial Services   \n",
       "1  2025-07-14 14:09:57  2025-07-14 14:21:54       716       Manufacturing   \n",
       "2  2025-07-14 14:09:57  2025-07-14 14:21:54       716       Manufacturing   \n",
       "3  2025-07-14 14:09:57  2025-07-14 14:21:54       716       Manufacturing   \n",
       "4  2025-07-14 14:09:57  2025-07-14 14:21:54       716       Manufacturing   \n",
       "\n",
       "       country  region  bps_average  bps_pct95  bps_max  pps_average  \\\n",
       "0       France  Europe          NaN        NaN      NaN          NaN   \n",
       "1  Switzerland  Europe          NaN        NaN      NaN          NaN   \n",
       "2  Switzerland  Europe          NaN        NaN      NaN          NaN   \n",
       "3  Switzerland  Europe          NaN        NaN      NaN          NaN   \n",
       "4  Switzerland  Europe          NaN        NaN      NaN          NaN   \n",
       "\n",
       "   pps_pct95  pps_max        vectors  \n",
       "0        NaN      NaN  Total Traffic  \n",
       "1        NaN      NaN  Total Traffic  \n",
       "2        NaN      NaN  Total Traffic  \n",
       "3        NaN      NaN  Total Traffic  \n",
       "4        NaN      NaN  Total Traffic  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(malicious_ips, attack_data, on='mitigation', how='inner')\n",
    "merged_df['date'] = pd.to_datetime(\n",
    "    merged_df['date'],\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    errors='coerce'\n",
    ")\n",
    "merged_df.drop(columns=['customers_y', 'Unnamed: 0_x', 'Unnamed: 0_y'], axis=1, inplace=True)\n",
    "merged_df = merged_df.rename(columns={'customers_x': 'customer'})\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2025-07-14      4\n",
       "2025-07-29      4\n",
       "2025-08-05      7\n",
       "2025-08-12     16\n",
       "2025-08-14      3\n",
       "2025-08-19      9\n",
       "2025-08-20      8\n",
       "2025-08-22     13\n",
       "2025-08-25     15\n",
       "2025-08-26      9\n",
       "2025-08-27      4\n",
       "2025-08-28     21\n",
       "2025-08-29     25\n",
       "2025-09-01      1\n",
       "2025-09-02      3\n",
       "2025-09-03     11\n",
       "2025-09-08      7\n",
       "2025-09-10      5\n",
       "2025-09-11     12\n",
       "2025-09-12     14\n",
       "2025-09-15     15\n",
       "2025-09-19      2\n",
       "2025-09-20      2\n",
       "2025-09-25      8\n",
       "2025-09-26      1\n",
       "2025-09-28      4\n",
       "2025-09-29    777\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_per_day = merged_df['date'].dt.date.value_counts().sort_index()\n",
    "events_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"2025-09-06 00:00:00\"\n",
    "end_time   = \"2025-09-06 23:59:59\"\n",
    "\n",
    "merged_df = merged_df[\n",
    "    (merged_df['date'] >= start_time) &\n",
    "    (merged_df['date'] <= end_time)\n",
    "]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Previous Cluster class unchanged]\n",
    "class Cluster:\n",
    "    def __init__(self, cluster_id, rows, logger):\n",
    "        self.cluster_id = cluster_id\n",
    "        self.rows = rows\n",
    "        self.logger = logger\n",
    "        self.ip_last_observed = {}\n",
    "        for row in rows:\n",
    "            try:\n",
    "                ip = str(row['Malicious_IP'])\n",
    "                date = row['date'].strftime('%Y-%m-%d')\n",
    "                self.ip_last_observed[ip] = date\n",
    "            except KeyError as e:\n",
    "                self.logger.error(f\"Missing required field in row: {e}, row: {row}\")\n",
    "            except AttributeError as e:\n",
    "                self.logger.error(f\"Invalid date format in row: {e}, row: {row}\")\n",
    "        self.logger.debug(f\"Cluster {self.cluster_id} initialized with {len(self.ip_last_observed)} IPs: {list(self.ip_last_observed.keys())}\")\n",
    "\n",
    "    def get_malicious_ips(self):\n",
    "        return {str(row['Malicious_IP']) for row in self.rows}\n",
    "\n",
    "    def extend(self, new_rows):\n",
    "        self.rows.extend(new_rows)\n",
    "        for row in new_rows:\n",
    "            try:\n",
    "                ip = str(row['Malicious_IP'])\n",
    "                date = row['date'].strftime('%Y-%m-%d')\n",
    "                if ip in self.ip_last_observed:\n",
    "                    self.ip_last_observed[ip] = max(self.ip_last_observed[ip], date)\n",
    "                else:\n",
    "                    self.ip_last_observed[ip] = date\n",
    "            except KeyError as e:\n",
    "                self.logger.error(f\"Missing required field in row: {e}, row: {row}\")\n",
    "            except AttributeError as e:\n",
    "                self.logger.error(f\"Invalid date format in row: {e}, row: {row}\")\n",
    "        self.logger.debug(f\"Cluster {self.cluster_id} extended, now has {len(self.ip_last_observed)} IPs: {list(self.ip_last_observed.keys())}\")\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        durations = [row['duration'] for row in self.rows if pd.notna(row['duration'])]\n",
    "        bps_avg = [row['bps_average'] / 1_000_000_000 for row in self.rows if pd.notna(row['bps_average'])]\n",
    "        bps_pct95 = [row['bps_pct95'] / 1_000_000_000 for row in self.rows if pd.notna(row['bps_pct95'])]\n",
    "        bps_max = [row['bps_max'] / 1_000_000_000 for row in self.rows if pd.notna(row['bps_max'])]\n",
    "        pps_avg = [row['pps_average'] for row in self.rows if pd.notna(row['pps_average'])]\n",
    "        pps_pct95 = [row['pps_pct95'] for row in self.rows if pd.notna(row['pps_pct95'])]\n",
    "        pps_max = [row['pps_max'] for row in self.rows if pd.notna(row['pps_max'])]\n",
    "\n",
    "        vectors_set = set()\n",
    "        for row in self.rows:\n",
    "            if pd.notna(row['vectors']):\n",
    "                vectors_set.update(row['vectors'].replace('\\/', '/').split(','))\n",
    "\n",
    "        return {\n",
    "            \"duration_max\": max(durations) if durations else None,\n",
    "            \"duration_avg\": sum(durations) / len(durations) if durations else None,\n",
    "            \"gbps_max\": max(bps_max) if bps_max else None,\n",
    "            \"gbps_avg\": sum(bps_avg) / len(bps_avg) if bps_avg else None,\n",
    "            \"gbps_pct95\": max(bps_pct95) if bps_pct95 else None,\n",
    "            \"pps_max\": max(pps_max) if pps_max else None,\n",
    "            \"pps_avg\": sum(pps_avg) / len(pps_avg) if pps_avg else None,\n",
    "            \"pps_pct95\": max(pps_pct95) if pps_pct95 else None,\n",
    "            \"vectors\": [vector.replace('\\/', '/') for vector in vectors_set]\n",
    "        }\n",
    "\n",
    "class BotnetClusterProcessor:\n",
    "    def __init__(self, df, analysis_file: str = 'cluster_analysis.json', \n",
    "                 ip_file: str = 'cluster_malicious_ips.json', \n",
    "                 ip_history_file: str = 'cluster_ip_history.json'):\n",
    "        required_columns = ['Malicious_IP', 'date', 'customer', 'industry', 'country', 'region', \n",
    "                           'duration', 'bps_average', 'bps_pct95', 'bps_max', \n",
    "                           'pps_average', 'pps_pct95', 'pps_max', 'vectors']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Input DataFrame missing required columns: {missing_columns}\")\n",
    "\n",
    "        self.df = df.copy()\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'], errors='coerce')\n",
    "        if self.df['date'].isna().any():\n",
    "            self.logger.warning(\"Some 'date' values could not be converted to datetime. NaT values introduced.\")\n",
    "        \n",
    "        self.clusters = []\n",
    "        self.analysis_file = analysis_file\n",
    "        self.ip_file = ip_file\n",
    "        self.ip_history_file = ip_history_file\n",
    "        self.logger = self._setup_logger()\n",
    "        self.prev_clusters = self._load_previous_clusters_from_json()\n",
    "        self.ip_cluster_history = self._load_ip_history_from_json()\n",
    "        self.ip_cluster_tracker = {}\n",
    "        self.first_observed_dates = {}\n",
    "        self.last_observed_dates = {}\n",
    "        self.active_status = {}\n",
    "        self.observation_counts = {}\n",
    "        self.ip_observation_counts = {}\n",
    "        self._initialize_ip_observation_counts()\n",
    "        self.next_cluster_id = max([c['ClusterID'] for c in self.prev_clusters], default=0) + 1\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        if not logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "            logger.setLevel(logging.INFO)\n",
    "        return logger\n",
    "\n",
    "    def _load_previous_clusters_from_json(self):\n",
    "        try:\n",
    "            try:\n",
    "                with open(self.analysis_file, 'r') as f:\n",
    "                    clusters = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                clusters = []\n",
    "                self.logger.info(f\"{self.analysis_file} not found, starting with empty clusters\")\n",
    "\n",
    "            try:\n",
    "                with open(self.ip_file, 'r') as f:\n",
    "                    ip_data = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                ip_data = []\n",
    "                self.logger.info(f\"{self.ip_file} not found, starting with empty IP data\")\n",
    "\n",
    "            for cluster in clusters:\n",
    "                cluster_id = cluster['ClusterID']\n",
    "                cluster['Malicious_IPs'] = [\n",
    "                    {\n",
    "                        'IP': item['Malicious_IP'],\n",
    "                        'Last_Observed': item['Last_Observed'],\n",
    "                        'First_Observed': item.get('First_Observed', '0000-01-01'),\n",
    "                        'Observation_Count': item.get('Observation_Count', 0)\n",
    "                    }\n",
    "                    for item in ip_data if cluster_id in item['Clusters']\n",
    "                ]\n",
    "            return clusters\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading previous clusters from JSON: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _load_ip_history_from_json(self):\n",
    "        try:\n",
    "            try:\n",
    "                with open(self.ip_history_file, 'r') as f:\n",
    "                    ip_history = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                ip_history = []\n",
    "                self.logger.info(f\"{self.ip_history_file} not found, starting with empty IP history\")\n",
    "\n",
    "            ip_history_dict = {}\n",
    "            for item in ip_history:\n",
    "                ip = item['IP']\n",
    "                entry = {'ClusterID': int(item['ClusterID']), 'Date': item['Date']}\n",
    "                if ip not in ip_history_dict:\n",
    "                    ip_history_dict[ip] = []\n",
    "                ip_history_dict[ip].append(entry)\n",
    "                if ip not in self.ip_cluster_tracker:\n",
    "                    self.ip_cluster_tracker[ip] = (entry['Date'], entry['ClusterID'])\n",
    "                else:\n",
    "                    current_date, _ = self.ip_cluster_tracker[ip]\n",
    "                    if entry['Date'] > current_date:\n",
    "                        self.ip_cluster_tracker[ip] = (entry['Date'], entry['ClusterID'])\n",
    "            return ip_history_dict\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading IP history from JSON: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _initialize_ip_observation_counts(self):\n",
    "        ip_counts = self.df['Malicious_IP'].value_counts().to_dict()\n",
    "        for ip, count in ip_counts.items():\n",
    "            self.ip_observation_counts[str(ip)] = count\n",
    "        self.logger.debug(f\"Initialized observation counts for {len(self.ip_observation_counts)} IPs\")\n",
    "\n",
    "    def is_cluster_similar(self, cluster_ips, new_ips, threshold=0.5):\n",
    "        intersection_size = len(cluster_ips.intersection(new_ips))\n",
    "        if not cluster_ips or not new_ips:\n",
    "            return False\n",
    "        similarity_ratio = intersection_size / min(len(cluster_ips), len(new_ips))\n",
    "        return similarity_ratio >= threshold\n",
    "\n",
    "    def process(self):\n",
    "        self.df = self.df.sort_values(by=['customer', 'date'])\n",
    "        current_cluster_rows = []\n",
    "        last_row = None\n",
    "\n",
    "        for _, row in self.df.iterrows():\n",
    "            if not current_cluster_rows:\n",
    "                current_cluster_rows.append(row)\n",
    "                last_row = row\n",
    "            else:\n",
    "                if (row['customer'] == last_row['customer'] and \n",
    "                    pd.notna(row['date']) and pd.notna(last_row['date']) and\n",
    "                    (row['date'] - last_row['date']).total_seconds() <= 300):\n",
    "                    current_cluster_rows.append(row)\n",
    "                    last_row = row\n",
    "                else:\n",
    "                    if current_cluster_rows:\n",
    "                        self._merge_or_add_cluster(current_cluster_rows)\n",
    "                    current_cluster_rows = [row]\n",
    "                    last_row = row\n",
    "\n",
    "        if current_cluster_rows:\n",
    "            self._merge_or_add_cluster(current_cluster_rows)\n",
    "\n",
    "        self._merge_similar_clusters()\n",
    "\n",
    "    def _merge_or_add_cluster(self, rows):\n",
    "        if not rows:\n",
    "            self.logger.warning(\"No rows provided to merge or add cluster\")\n",
    "            return\n",
    "\n",
    "        new_ips = {str(row['Malicious_IP']) for row in rows if 'Malicious_IP' in row}\n",
    "        if not new_ips:\n",
    "            self.logger.warning(\"No valid IPs found in rows to merge or add cluster\")\n",
    "            return\n",
    "\n",
    "        first_observed = min(row['date'] for row in rows if pd.notna(row['date']))\n",
    "        last_observed = max(row['date'] for row in rows if pd.notna(row['date']))\n",
    "        merged = False\n",
    "\n",
    "        for prev_cluster in self.prev_clusters:\n",
    "            prev_ips = {ip_entry['IP'] for ip_entry in prev_cluster.get('Malicious_IPs', [])}\n",
    "            if self.is_cluster_similar(prev_ips, new_ips):\n",
    "                prev_cluster['First_Observed'] = min(\n",
    "                    pd.to_datetime(prev_cluster.get('First_Observed', first_observed)),\n",
    "                    first_observed\n",
    "                ).strftime('%Y-%m-%d')\n",
    "                prev_cluster['Last_Observed'] = max(\n",
    "                    pd.to_datetime(prev_cluster.get('Last_Observed', last_observed)),\n",
    "                    last_observed\n",
    "                ).strftime('%Y-%m-%d')\n",
    "                prev_cluster['Active'] = 'Yes' if (pd.Timestamp.now() - pd.to_datetime(prev_cluster['Last_Observed'])).days <= 14 else 'No'\n",
    "                prev_cluster['Observations'] = prev_cluster.get('Observations', 0) + len(rows)\n",
    "\n",
    "                ip_dates = {ip_entry['IP']: ip_entry['Last_Observed'] for ip_entry in prev_cluster.get('Malicious_IPs', [])}\n",
    "                ip_counts = {ip_entry['IP']: ip_entry.get('Observation_Count', 0) for ip_entry in prev_cluster.get('Malicious_IPs', [])}\n",
    "                for row in rows:\n",
    "                    ip = str(row['Malicious_IP'])\n",
    "                    date = row['date'].strftime('%Y-%m-%d')\n",
    "                    if ip in ip_dates:\n",
    "                        ip_dates[ip] = max(ip_dates[ip], date)\n",
    "                        ip_counts[ip] += 1\n",
    "                    else:\n",
    "                        ip_dates[ip] = date\n",
    "                        ip_counts[ip] = self.ip_observation_counts.get(ip, 1)\n",
    "                prev_cluster['Malicious_IPs'] = [\n",
    "                    {\"IP\": ip, \"Last_Observed\": date, \"First_Observed\": min(ip_dates[ip], prev_cluster.get('First_Observed', date)), \"Observation_Count\": ip_counts[ip]}\n",
    "                    for ip, date in ip_dates.items()\n",
    "                ]\n",
    "\n",
    "                prev_cluster['customer'] = list(set(prev_cluster.get('customer', [])).union({row['customer'] for row in rows}))\n",
    "                prev_cluster['customer_count'] = len(prev_cluster['customer'])\n",
    "                prev_cluster['Industries'] = list(set(prev_cluster.get('Industries', [])).union({row['industry'] for row in rows}))\n",
    "                prev_cluster['Industry_Count'] = len(prev_cluster['Industries'])\n",
    "                prev_cluster['Countries'] = list(set(prev_cluster.get('Countries', [])).union({row['country'] for row in rows}))\n",
    "                prev_cluster['Country_Count'] = len(prev_cluster['Countries'])\n",
    "                prev_cluster['Regions'] = list(set(prev_cluster.get('Regions', [])).union({row['region'] for row in rows}))\n",
    "                prev_cluster['Region_Count'] = len(prev_cluster['Regions'])\n",
    "\n",
    "                computed = Cluster(0, rows, self.logger).compute_metrics()\n",
    "                for key in ['duration_max', 'gbps_max', 'gbps_pct95', 'pps_max', 'pps_pct95']:\n",
    "                    if computed[key] is not None and prev_cluster.get(key) is not None:\n",
    "                        prev_cluster[key] = max(prev_cluster[key], computed[key])\n",
    "                    elif computed[key] is not None:\n",
    "                        prev_cluster[key] = computed[key]\n",
    "                for key in ['duration_avg', 'gbps_avg', 'pps_avg']:\n",
    "                    if computed[key] is not None:\n",
    "                        prev_value = prev_cluster.get(key, 0) if prev_cluster.get(key) is not None else 0\n",
    "                        prev_observations = prev_cluster.get('Observations', 1)\n",
    "                        prev_cluster[key] = (\n",
    "                            (prev_value * prev_observations + computed[key] * len(rows)) / \n",
    "                            (prev_observations + len(rows))\n",
    "                        )\n",
    "                prev_cluster['Vectors'] = list(set(prev_cluster.get('Vectors', [])).union(computed['vectors']))\n",
    "                prev_cluster['Cluster_Size'] = len(ip_dates)\n",
    "                merged = True\n",
    "                break\n",
    "\n",
    "        if not merged:\n",
    "            new_cluster = Cluster(self.next_cluster_id, rows, self.logger)\n",
    "            self.clusters.append(new_cluster)\n",
    "            self.first_observed_dates[self.next_cluster_id] = first_observed\n",
    "            self.last_observed_dates[self.next_cluster_id] = last_observed\n",
    "            self.active_status[self.next_cluster_id] = 'Yes' if (pd.Timestamp.now() - last_observed).days <= 14 else 'No'\n",
    "            self.observation_counts[self.next_cluster_id] = len(rows)\n",
    "            self.next_cluster_id += 1\n",
    "\n",
    "    def _merge_similar_clusters(self):\n",
    "        merged_clusters = []\n",
    "        used_cluster_ids = set()\n",
    "\n",
    "        for i, cluster_i in enumerate(self.clusters):\n",
    "            if cluster_i.cluster_id in used_cluster_ids:\n",
    "                continue\n",
    "            cluster_i_ips = cluster_i.get_malicious_ips()\n",
    "            merged_rows = cluster_i.rows.copy()\n",
    "            used_cluster_ids.add(cluster_i.cluster_id)\n",
    "\n",
    "            for j, cluster_j in enumerate(self.clusters[i+1:], start=i+1):\n",
    "                if cluster_j.cluster_id in used_cluster_ids:\n",
    "                    continue\n",
    "                cluster_j_ips = cluster_j.get_malicious_ips()\n",
    "                if self.is_cluster_similar(cluster_i_ips, cluster_j_ips):\n",
    "                    merged_rows.extend(cluster_j.rows)\n",
    "                    for ip, date in cluster_j.ip_last_observed.items():\n",
    "                        if ip in cluster_i.ip_last_observed:\n",
    "                            cluster_i.ip_last_observed[ip] = max(cluster_i.ip_last_observed[ip], date)\n",
    "                        else:\n",
    "                            cluster_i.ip_last_observed[ip] = date\n",
    "                    used_cluster_ids.add(cluster_j.cluster_id)\n",
    "                    cluster_i_ips = cluster_i_ips.union(cluster_j_ips)\n",
    "\n",
    "            merged_cluster = Cluster(cluster_i.cluster_id, merged_rows, self.logger)\n",
    "            merged_cluster.ip_last_observed = cluster_i.ip_last_observed\n",
    "            merged_clusters.append(merged_cluster)\n",
    "\n",
    "        self.clusters = merged_clusters\n",
    "\n",
    "    def _clean_nan(self, obj):\n",
    "        \"\"\"Recursively replace NaN values with None for JSON compatibility.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self._clean_nan(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._clean_nan(v) for v in obj]\n",
    "        elif isinstance(obj, float) and np.isnan(obj):\n",
    "            return None\n",
    "        return obj\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Save the clustering results to JSON files, handling NaN values.\"\"\"\n",
    "        all_clusters = []\n",
    "        seen_cluster_ids = set()\n",
    "\n",
    "        for cluster in self.prev_clusters:\n",
    "            if cluster['ClusterID'] not in seen_cluster_ids:\n",
    "                all_clusters.append(cluster)\n",
    "                seen_cluster_ids.add(cluster['ClusterID'])\n",
    "\n",
    "        for cluster in self.clusters:\n",
    "            if cluster.cluster_id not in seen_cluster_ids:\n",
    "                metrics = cluster.compute_metrics()\n",
    "                all_clusters.append({\n",
    "                    'ClusterID': cluster.cluster_id,\n",
    "                    'duration_max': metrics['duration_max'],\n",
    "                    'duration_avg': metrics['duration_avg'],\n",
    "                    'gbps_max': metrics['gbps_max'],\n",
    "                    'gbps_avg': metrics['gbps_avg'],\n",
    "                    'gbps_pct95': metrics['gbps_pct95'],\n",
    "                    'pps_max': metrics['pps_max'],\n",
    "                    'pps_avg': metrics['pps_avg'],\n",
    "                    'pps_pct95': metrics['pps_pct95'],\n",
    "                    'customer': list({row['customer'] for row in cluster.rows}),\n",
    "                    'customer_count': len({row['customer'] for row in cluster.rows}),\n",
    "                    'Industries': list({row['industry'] for row in cluster.rows}),\n",
    "                    'Industry_Count': len({row['industry'] for row in cluster.rows}),\n",
    "                    'Countries': list({row['country'] for row in cluster.rows}),\n",
    "                    'Country_Count': len({row['country'] for row in cluster.rows}),\n",
    "                    'Regions': list({row['region'] for row in cluster.rows}),\n",
    "                    'Region_Count': len({row['region'] for row in cluster.rows}),\n",
    "                    'Observations': self.observation_counts.get(cluster.cluster_id, len(cluster.rows)),\n",
    "                    'Vectors': metrics['vectors'],\n",
    "                    'First_Observed': self.first_observed_dates.get(cluster.cluster_id).strftime('%Y-%m-%d'),\n",
    "                    'Last_Observed': self.last_observed_dates.get(cluster.cluster_id).strftime('%Y-%m-%d'),\n",
    "                    'Active': self.active_status.get(cluster.cluster_id),\n",
    "                    'Malicious_IPs': [\n",
    "                        {\"IP\": ip, \"Last_Observed\": date, \"First_Observed\": date, \"Observation_Count\": self.ip_observation_counts.get(ip, 1)}\n",
    "                        for ip, date in cluster.ip_last_observed.items()\n",
    "                    ],\n",
    "                    'Cluster_Size': len(cluster.ip_last_observed)\n",
    "                })\n",
    "                seen_cluster_ids.add(cluster.cluster_id)\n",
    "\n",
    "        latest_date = max(cluster['Last_Observed'] for cluster in all_clusters) if all_clusters else datetime.now().strftime('%Y-%m-%d')\n",
    "        for cluster in all_clusters:\n",
    "            cluster_id = cluster['ClusterID']\n",
    "            for ip_entry in cluster['Malicious_IPs']:\n",
    "                ip = ip_entry['IP']\n",
    "                if ip not in self.ip_cluster_history:\n",
    "                    self.ip_cluster_history[ip] = []\n",
    "                if ip in self.ip_cluster_tracker:\n",
    "                    last_date, last_cluster_id = self.ip_cluster_tracker[ip]\n",
    "                    if last_date == latest_date and last_cluster_id == cluster_id:\n",
    "                        continue\n",
    "                    elif last_cluster_id != cluster_id:\n",
    "                        self.ip_cluster_history[ip].append({\"ClusterID\": cluster_id, \"Date\": latest_date})\n",
    "                        self.ip_cluster_tracker[ip] = (latest_date, cluster_id)\n",
    "                else:\n",
    "                    self.ip_cluster_history[ip].append({\"ClusterID\": cluster_id, \"Date\": latest_date})\n",
    "                    self.ip_cluster_tracker[ip] = (latest_date, cluster_id)\n",
    "\n",
    "        analysis_data = [{k: v for k, v in cluster.items() if k != 'Malicious_IPs'} for cluster in all_clusters]\n",
    "\n",
    "        ip_last_cluster = {}\n",
    "        ip_first_observed = {}\n",
    "        ip_last_observed = {}\n",
    "        for cluster in all_clusters:\n",
    "            cluster_id = cluster['ClusterID']\n",
    "            for ip_entry in cluster['Malicious_IPs']:\n",
    "                ip = ip_entry['IP']\n",
    "                current_observed = ip_entry.get('Last_Observed')\n",
    "                if current_observed:\n",
    "                    if ip not in ip_first_observed or current_observed < ip_first_observed[ip]:\n",
    "                        ip_first_observed[ip] = current_observed\n",
    "                    if ip not in ip_last_observed or current_observed > ip_last_observed[ip]:\n",
    "                        ip_last_observed[ip] = current_observed\n",
    "                    if ip not in ip_last_cluster or current_observed >= ip_last_cluster[ip][1]:\n",
    "                        ip_last_cluster[ip] = (cluster_id, current_observed)\n",
    "\n",
    "        ip_data = [\n",
    "            {\n",
    "                'Malicious_IP': ip,\n",
    "                'Clusters': [cluster_id],\n",
    "                'First_Observed': ip_first_observed[ip],\n",
    "                'Last_Observed': ip_last_observed[ip],\n",
    "                'Observation_Count': self.ip_observation_counts.get(ip, 1)\n",
    "            }\n",
    "            for ip, (cluster_id, _) in ip_last_cluster.items()\n",
    "        ]\n",
    "\n",
    "        ip_history_items = [\n",
    "            {'IP': ip, 'Date': entry['Date'], 'ClusterID': entry['ClusterID']}\n",
    "            for ip, history in self.ip_cluster_history.items()\n",
    "            for entry in history\n",
    "        ]\n",
    "\n",
    "        # Clean NaN values\n",
    "        analysis_data = self._clean_nan(analysis_data)\n",
    "        ip_data = self._clean_nan(ip_data)\n",
    "        ip_history_items = self._clean_nan(ip_history_items)\n",
    "\n",
    "        # Save to JSON files\n",
    "        try:\n",
    "            with open(self.analysis_file, 'w') as f:\n",
    "                json.dump(analysis_data, f, indent=4)\n",
    "            self.logger.info(f\"Saved {len(analysis_data)} clusters to {self.analysis_file}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save to {self.analysis_file}: {e}\")\n",
    "\n",
    "        try:\n",
    "            with open(self.ip_file, 'w') as f:\n",
    "                json.dump(ip_data, f, indent=4)\n",
    "            self.logger.info(f\"Saved {len(ip_data)} IPs to {self.ip_file}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save to {self.ip_file}: {e}\")\n",
    "\n",
    "        try:\n",
    "            with open(self.ip_history_file, 'w') as f:\n",
    "                json.dump(ip_history_items, f, indent=4)\n",
    "            self.logger.info(f\"Saved {len(ip_history_items)} IP history entries to {self.ip_history_file}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save to {self.ip_history_file}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(all_clusters)\n",
    "        df['Last_Observed'] = pd.to_datetime(df['Last_Observed'], errors='coerce')\n",
    "        df['First_Observed'] = pd.to_datetime(df['First_Observed'], errors='coerce')\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = BotnetClusterProcessor(\n",
    "        merged_df,\n",
    "        analysis_file='cluster_analysis.json',\n",
    "        ip_file='cluster_malicious_ips.json',\n",
    "        ip_history_file='cluster_ip_history.json'\n",
    "    )\n",
    "    processor.process()\n",
    "    cluster_df = processor.save_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
