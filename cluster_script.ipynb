{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import timedelta, datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_ips = pd.read_csv(\"Malicious_ips.csv\")\n",
    "malicious_ips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_data = pd.read_csv(\"attack_data.csv\")\n",
    "attack_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(malicious_ips, attack_data, on='mitigation', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(malicious_ips, attack_data, on='mitigation', how='inner')\n",
    "merged_df['date'] = pd.to_datetime(\n",
    "    merged_df['date'],\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    errors='coerce'\n",
    ")\n",
    "merged_df.drop(columns=['customers_y', 'Unnamed: 0_x', 'Unnamed: 0_y'], axis=1, inplace=True)\n",
    "merged_df = merged_df.rename(columns={'customers_x': 'customer'})\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_per_day = merged_df['date'].dt.date.value_counts().sort_index()\n",
    "events_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"2025-09-29 00:00:00\"\n",
    "end_time   = \"2025-09-29 23:59:59\"\n",
    "\n",
    "merged_df = merged_df[\n",
    "    (merged_df['date'] >= start_time) &\n",
    "    (merged_df['date'] <= end_time)\n",
    "]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Previous Cluster class]\n",
    "class Cluster:\n",
    "    def __init__(self, cluster_id, rows, logger):\n",
    "        self.cluster_id = cluster_id\n",
    "        self.rows = rows\n",
    "        self.logger = logger\n",
    "        self.ip_last_observed = {}\n",
    "        for row in rows:\n",
    "            try:\n",
    "                ip = str(row['Malicious_IP'])\n",
    "                date = row['date'].strftime('%Y-%m-%d')\n",
    "                self.ip_last_observed[ip] = date\n",
    "            except KeyError as e:\n",
    "                self.logger.error(f\"Missing required field in row: {e}, row: {row}\")\n",
    "            except AttributeError as e:\n",
    "                self.logger.error(f\"Invalid date format in row: {e}, row: {row}\")\n",
    "        self.logger.debug(f\"Cluster {self.cluster_id} initialized with {len(self.ip_last_observed)} IPs: {list(self.ip_last_observed.keys())}\")\n",
    "\n",
    "    def get_malicious_ips(self):\n",
    "        return {str(row['Malicious_IP']) for row in self.rows}\n",
    "\n",
    "    def extend(self, new_rows):\n",
    "        self.rows.extend(new_rows)\n",
    "        for row in new_rows:\n",
    "            try:\n",
    "                ip = str(row['Malicious_IP'])\n",
    "                date = row['date'].strftime('%Y-%m-%d')\n",
    "                if ip in self.ip_last_observed:\n",
    "                    self.ip_last_observed[ip] = max(self.ip_last_observed[ip], date)\n",
    "                else:\n",
    "                    self.ip_last_observed[ip] = date\n",
    "            except KeyError as e:\n",
    "                self.logger.error(f\"Missing required field in row: {e}, row: {row}\")\n",
    "            except AttributeError as e:\n",
    "                self.logger.error(f\"Invalid date format in row: {e}, row: {row}\")\n",
    "        self.logger.debug(f\"Cluster {self.cluster_id} extended, now has {len(self.ip_last_observed)} IPs: {list(self.ip_last_observed.keys())}\")\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        durations = [row['duration'] for row in self.rows if pd.notna(row['duration'])]\n",
    "        bps_avg = [row['bps_average'] / 1_000_000_000 for row in self.rows if pd.notna(row['bps_average'])]\n",
    "        bps_pct95 = [row['bps_pct95'] / 1_000_000_000 for row in self.rows if pd.notna(row['bps_pct95'])]\n",
    "        bps_max = [row['bps_max'] / 1_000_000_000 for row in self.rows if pd.notna(row['bps_max'])]\n",
    "        pps_avg = [row['pps_average'] for row in self.rows if pd.notna(row['pps_average'])]\n",
    "        pps_pct95 = [row['pps_pct95'] for row in self.rows if pd.notna(row['pps_pct95'])]\n",
    "        pps_max = [row['pps_max'] for row in self.rows if pd.notna(row['pps_max'])]\n",
    "\n",
    "        vectors_set = set()\n",
    "        for row in self.rows:\n",
    "            if pd.notna(row['vectors']):\n",
    "                vectors_set.update(row['vectors'].replace('\\/', '/').split(','))\n",
    "\n",
    "        return {\n",
    "            \"duration_max\": max(durations) if durations else None,\n",
    "            \"duration_avg\": sum(durations) / len(durations) if durations else None,\n",
    "            \"gbps_max\": max(bps_max) if bps_max else None,\n",
    "            \"gbps_avg\": sum(bps_avg) / len(bps_avg) if bps_avg else None,\n",
    "            \"gbps_pct95\": max(bps_pct95) if bps_pct95 else None,\n",
    "            \"pps_max\": max(pps_max) if pps_max else None,\n",
    "            \"pps_avg\": sum(pps_avg) / len(pps_avg) if pps_avg else None,\n",
    "            \"pps_pct95\": max(pps_pct95) if pps_pct95 else None,\n",
    "            \"vectors\": [vector.replace('\\/', '/') for vector in vectors_set]\n",
    "        }\n",
    "\n",
    "class BotnetClusterProcessor:\n",
    "    def __init__(self, df, analysis_file: str = 'cluster_analysis.json', \n",
    "                 ip_file: str = 'cluster_malicious_ips.json', \n",
    "                 ip_history_file: str = 'cluster_ip_history.json'):\n",
    "        required_columns = ['Malicious_IP', 'date', 'customer', 'industry', 'country', 'region', \n",
    "                           'duration', 'bps_average', 'bps_pct95', 'bps_max', \n",
    "                           'pps_average', 'pps_pct95', 'pps_max', 'vectors']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Input DataFrame missing required columns: {missing_columns}\")\n",
    "\n",
    "        self.df = df.copy()\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'], errors='coerce')\n",
    "        if self.df['date'].isna().any():\n",
    "            self.logger.warning(\"Some 'date' values could not be converted to datetime. NaT values introduced.\")\n",
    "        \n",
    "        self.clusters = []\n",
    "        self.analysis_file = analysis_file\n",
    "        self.ip_file = ip_file\n",
    "        self.ip_history_file = ip_history_file\n",
    "        self.logger = self._setup_logger()\n",
    "        self.prev_clusters = self._load_previous_clusters_from_json()\n",
    "        self.ip_cluster_history = self._load_ip_history_from_json()\n",
    "        self.ip_cluster_tracker = {}\n",
    "        self.first_observed_dates = {}\n",
    "        self.last_observed_dates = {}\n",
    "        self.active_status = {}\n",
    "        self.observation_counts = {}\n",
    "        self.ip_observation_counts = {}\n",
    "        self._initialize_ip_observation_counts()\n",
    "        self.next_cluster_id = max([c['ClusterID'] for c in self.prev_clusters], default=0) + 1\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        if not logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "            logger.setLevel(logging.INFO)\n",
    "        return logger\n",
    "\n",
    "    def _load_previous_clusters_from_json(self):\n",
    "        try:\n",
    "            try:\n",
    "                with open(self.analysis_file, 'r') as f:\n",
    "                    clusters = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                clusters = []\n",
    "                self.logger.info(f\"{self.analysis_file} not found, starting with empty clusters\")\n",
    "\n",
    "            try:\n",
    "                with open(self.ip_file, 'r') as f:\n",
    "                    ip_data = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                ip_data = []\n",
    "                self.logger.info(f\"{self.ip_file} not found, starting with empty IP data\")\n",
    "\n",
    "            for cluster in clusters:\n",
    "                cluster_id = cluster['ClusterID']\n",
    "                cluster['Malicious_IPs'] = [\n",
    "                    {\n",
    "                        'IP': item['Malicious_IP'],\n",
    "                        'Last_Observed': item['Last_Observed'],\n",
    "                        'First_Observed': item.get('First_Observed', '0000-01-01'),\n",
    "                        'Observation_Count': item.get('Observation_Count', 0)\n",
    "                    }\n",
    "                    for item in ip_data if cluster_id in item['Clusters']\n",
    "                ]\n",
    "            return clusters\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading previous clusters from JSON: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _load_ip_history_from_json(self):\n",
    "        try:\n",
    "            try:\n",
    "                with open(self.ip_history_file, 'r') as f:\n",
    "                    ip_history = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                ip_history = []\n",
    "                self.logger.info(f\"{self.ip_history_file} not found, starting with empty IP history\")\n",
    "\n",
    "            ip_history_dict = {}\n",
    "            for item in ip_history:\n",
    "                ip = item['IP']\n",
    "                entry = {'ClusterID': int(item['ClusterID']), 'Date': item['Date']}\n",
    "                if ip not in ip_history_dict:\n",
    "                    ip_history_dict[ip] = []\n",
    "                ip_history_dict[ip].append(entry)\n",
    "                if ip not in self.ip_cluster_tracker:\n",
    "                    self.ip_cluster_tracker[ip] = (entry['Date'], entry['ClusterID'])\n",
    "                else:\n",
    "                    current_date, _ = self.ip_cluster_tracker[ip]\n",
    "                    if entry['Date'] > current_date:\n",
    "                        self.ip_cluster_tracker[ip] = (entry['Date'], entry['ClusterID'])\n",
    "            return ip_history_dict\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading IP history from JSON: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _initialize_ip_observation_counts(self):\n",
    "        ip_counts = self.df['Malicious_IP'].value_counts().to_dict()\n",
    "        for ip, count in ip_counts.items():\n",
    "            self.ip_observation_counts[str(ip)] = count\n",
    "        self.logger.debug(f\"Initialized observation counts for {len(self.ip_observation_counts)} IPs\")\n",
    "\n",
    "    def is_cluster_similar(self, cluster_ips, new_ips, threshold=0.5):\n",
    "        intersection_size = len(cluster_ips.intersection(new_ips))\n",
    "        if not cluster_ips or not new_ips:\n",
    "            return False\n",
    "        similarity_ratio = intersection_size / min(len(cluster_ips), len(new_ips))\n",
    "        return similarity_ratio >= threshold\n",
    "\n",
    "    def process(self):\n",
    "        self.df = self.df.sort_values(by=['customer', 'date'])\n",
    "        current_cluster_rows = []\n",
    "        last_row = None\n",
    "\n",
    "        for _, row in self.df.iterrows():\n",
    "            if not current_cluster_rows:\n",
    "                current_cluster_rows.append(row)\n",
    "                last_row = row\n",
    "            else:\n",
    "                if (row['customer'] == last_row['customer'] and \n",
    "                    pd.notna(row['date']) and pd.notna(last_row['date']) and\n",
    "                    (row['date'] - last_row['date']).total_seconds() <= 300):\n",
    "                    current_cluster_rows.append(row)\n",
    "                    last_row = row\n",
    "                else:\n",
    "                    if current_cluster_rows:\n",
    "                        self._merge_or_add_cluster(current_cluster_rows)\n",
    "                    current_cluster_rows = [row]\n",
    "                    last_row = row\n",
    "\n",
    "        if current_cluster_rows:\n",
    "            self._merge_or_add_cluster(current_cluster_rows)\n",
    "\n",
    "        self._merge_similar_clusters()\n",
    "\n",
    "    def _merge_or_add_cluster(self, rows):\n",
    "        if not rows:\n",
    "            self.logger.warning(\"No rows provided to merge or add cluster\")\n",
    "            return\n",
    "\n",
    "        new_ips = {str(row['Malicious_IP']) for row in rows if 'Malicious_IP' in row}\n",
    "        if not new_ips:\n",
    "            self.logger.warning(\"No valid IPs found in rows to merge or add cluster\")\n",
    "            return\n",
    "\n",
    "        first_observed = min(row['date'] for row in rows if pd.notna(row['date']))\n",
    "        last_observed = max(row['date'] for row in rows if pd.notna(row['date']))\n",
    "        merged = False\n",
    "\n",
    "        for prev_cluster in self.prev_clusters:\n",
    "            prev_ips = {ip_entry['IP'] for ip_entry in prev_cluster.get('Malicious_IPs', [])}\n",
    "            if self.is_cluster_similar(prev_ips, new_ips):\n",
    "                prev_cluster['First_Observed'] = min(\n",
    "                    pd.to_datetime(prev_cluster.get('First_Observed', first_observed)),\n",
    "                    first_observed\n",
    "                ).strftime('%Y-%m-%d')\n",
    "                prev_cluster['Last_Observed'] = max(\n",
    "                    pd.to_datetime(prev_cluster.get('Last_Observed', last_observed)),\n",
    "                    last_observed\n",
    "                ).strftime('%Y-%m-%d')\n",
    "                prev_cluster['Active'] = 'Yes' if (pd.Timestamp.now() - pd.to_datetime(prev_cluster['Last_Observed'])).days <= 14 else 'No'\n",
    "                prev_cluster['Observations'] = prev_cluster.get('Observations', 0) + len(rows)\n",
    "\n",
    "                ip_dates = {ip_entry['IP']: ip_entry['Last_Observed'] for ip_entry in prev_cluster.get('Malicious_IPs', [])}\n",
    "                ip_counts = {ip_entry['IP']: ip_entry.get('Observation_Count', 0) for ip_entry in prev_cluster.get('Malicious_IPs', [])}\n",
    "                for row in rows:\n",
    "                    ip = str(row['Malicious_IP'])\n",
    "                    date = row['date'].strftime('%Y-%m-%d')\n",
    "                    if ip in ip_dates:\n",
    "                        ip_dates[ip] = max(ip_dates[ip], date)\n",
    "                        ip_counts[ip] += 1\n",
    "                    else:\n",
    "                        ip_dates[ip] = date\n",
    "                        ip_counts[ip] = self.ip_observation_counts.get(ip, 1)\n",
    "                prev_cluster['Malicious_IPs'] = [\n",
    "                    {\"IP\": ip, \"Last_Observed\": date, \"First_Observed\": min(ip_dates[ip], prev_cluster.get('First_Observed', date)), \"Observation_Count\": ip_counts[ip]}\n",
    "                    for ip, date in ip_dates.items()\n",
    "                ]\n",
    "\n",
    "                prev_cluster['customer'] = list(set(prev_cluster.get('customer', [])).union({row['customer'] for row in rows}))\n",
    "                prev_cluster['customer_count'] = len(prev_cluster['customer'])\n",
    "                prev_cluster['Industries'] = list(set(prev_cluster.get('Industries', [])).union({row['industry'] for row in rows}))\n",
    "                prev_cluster['Industry_Count'] = len(prev_cluster['Industries'])\n",
    "                prev_cluster['Countries'] = list(set(prev_cluster.get('Countries', [])).union({row['country'] for row in rows}))\n",
    "                prev_cluster['Country_Count'] = len(prev_cluster['Countries'])\n",
    "                prev_cluster['Regions'] = list(set(prev_cluster.get('Regions', [])).union({row['region'] for row in rows}))\n",
    "                prev_cluster['Region_Count'] = len(prev_cluster['Regions'])\n",
    "\n",
    "                computed = Cluster(0, rows, self.logger).compute_metrics()\n",
    "                for key in ['duration_max', 'gbps_max', 'gbps_pct95', 'pps_max', 'pps_pct95']:\n",
    "                    if computed[key] is not None and prev_cluster.get(key) is not None:\n",
    "                        prev_cluster[key] = max(prev_cluster[key], computed[key])\n",
    "                    elif computed[key] is not None:\n",
    "                        prev_cluster[key] = computed[key]\n",
    "                for key in ['duration_avg', 'gbps_avg', 'pps_avg']:\n",
    "                    if computed[key] is not None:\n",
    "                        prev_value = prev_cluster.get(key, 0) if prev_cluster.get(key) is not None else 0\n",
    "                        prev_observations = prev_cluster.get('Observations', 1)\n",
    "                        prev_cluster[key] = (\n",
    "                            (prev_value * prev_observations + computed[key] * len(rows)) / \n",
    "                            (prev_observations + len(rows))\n",
    "                        )\n",
    "                prev_cluster['Vectors'] = list(set(prev_cluster.get('Vectors', [])).union(computed['vectors']))\n",
    "                prev_cluster['Cluster_Size'] = len(ip_dates)\n",
    "                merged = True\n",
    "                break\n",
    "\n",
    "        if not merged:\n",
    "            new_cluster = Cluster(self.next_cluster_id, rows, self.logger)\n",
    "            self.clusters.append(new_cluster)\n",
    "            self.first_observed_dates[self.next_cluster_id] = first_observed\n",
    "            self.last_observed_dates[self.next_cluster_id] = last_observed\n",
    "            self.active_status[self.next_cluster_id] = 'Yes' if (pd.Timestamp.now() - last_observed).days <= 14 else 'No'\n",
    "            self.observation_counts[self.next_cluster_id] = len(rows)\n",
    "            self.next_cluster_id += 1\n",
    "\n",
    "    def _merge_similar_clusters(self):\n",
    "        merged_clusters = []\n",
    "        used_cluster_ids = set()\n",
    "\n",
    "        for i, cluster_i in enumerate(self.clusters):\n",
    "            if cluster_i.cluster_id in used_cluster_ids:\n",
    "                continue\n",
    "            cluster_i_ips = cluster_i.get_malicious_ips()\n",
    "            merged_rows = cluster_i.rows.copy()\n",
    "            used_cluster_ids.add(cluster_i.cluster_id)\n",
    "\n",
    "            for j, cluster_j in enumerate(self.clusters[i+1:], start=i+1):\n",
    "                if cluster_j.cluster_id in used_cluster_ids:\n",
    "                    continue\n",
    "                cluster_j_ips = cluster_j.get_malicious_ips()\n",
    "                if self.is_cluster_similar(cluster_i_ips, cluster_j_ips):\n",
    "                    merged_rows.extend(cluster_j.rows)\n",
    "                    for ip, date in cluster_j.ip_last_observed.items():\n",
    "                        if ip in cluster_i.ip_last_observed:\n",
    "                            cluster_i.ip_last_observed[ip] = max(cluster_i.ip_last_observed[ip], date)\n",
    "                        else:\n",
    "                            cluster_i.ip_last_observed[ip] = date\n",
    "                    used_cluster_ids.add(cluster_j.cluster_id)\n",
    "                    cluster_i_ips = cluster_i_ips.union(cluster_j_ips)\n",
    "\n",
    "            merged_cluster = Cluster(cluster_i.cluster_id, merged_rows, self.logger)\n",
    "            merged_cluster.ip_last_observed = cluster_i.ip_last_observed\n",
    "            merged_clusters.append(merged_cluster)\n",
    "\n",
    "        self.clusters = merged_clusters\n",
    "\n",
    "    def _clean_nan(self, obj):\n",
    "        \"\"\"Recursively replace NaN values with None for JSON compatibility.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self._clean_nan(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._clean_nan(v) for v in obj]\n",
    "        elif isinstance(obj, float) and np.isnan(obj):\n",
    "            return None\n",
    "        return obj\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Save the clustering results to JSON files, handling NaN values.\"\"\"\n",
    "        all_clusters = []\n",
    "        seen_cluster_ids = set()\n",
    "\n",
    "        for cluster in self.prev_clusters:\n",
    "            if cluster['ClusterID'] not in seen_cluster_ids:\n",
    "                all_clusters.append(cluster)\n",
    "                seen_cluster_ids.add(cluster['ClusterID'])\n",
    "\n",
    "        for cluster in self.clusters:\n",
    "            if cluster.cluster_id not in seen_cluster_ids:\n",
    "                metrics = cluster.compute_metrics()\n",
    "                all_clusters.append({\n",
    "                    'ClusterID': cluster.cluster_id,\n",
    "                    'duration_max': metrics['duration_max'],\n",
    "                    'duration_avg': metrics['duration_avg'],\n",
    "                    'gbps_max': metrics['gbps_max'],\n",
    "                    'gbps_avg': metrics['gbps_avg'],\n",
    "                    'gbps_pct95': metrics['gbps_pct95'],\n",
    "                    'pps_max': metrics['pps_max'],\n",
    "                    'pps_avg': metrics['pps_avg'],\n",
    "                    'pps_pct95': metrics['pps_pct95'],\n",
    "                    'customer': list({row['customer'] for row in cluster.rows}),\n",
    "                    'customer_count': len({row['customer'] for row in cluster.rows}),\n",
    "                    'Industries': list({row['industry'] for row in cluster.rows}),\n",
    "                    'Industry_Count': len({row['industry'] for row in cluster.rows}),\n",
    "                    'Countries': list({row['country'] for row in cluster.rows}),\n",
    "                    'Country_Count': len({row['country'] for row in cluster.rows}),\n",
    "                    'Regions': list({row['region'] for row in cluster.rows}),\n",
    "                    'Region_Count': len({row['region'] for row in cluster.rows}),\n",
    "                    'Observations': self.observation_counts.get(cluster.cluster_id, len(cluster.rows)),\n",
    "                    'Vectors': metrics['vectors'],\n",
    "                    'First_Observed': self.first_observed_dates.get(cluster.cluster_id).strftime('%Y-%m-%d'),\n",
    "                    'Last_Observed': self.last_observed_dates.get(cluster.cluster_id).strftime('%Y-%m-%d'),\n",
    "                    'Active': self.active_status.get(cluster.cluster_id),\n",
    "                    'Malicious_IPs': [\n",
    "                        {\"IP\": ip, \"Last_Observed\": date, \"First_Observed\": date, \"Observation_Count\": self.ip_observation_counts.get(ip, 1)}\n",
    "                        for ip, date in cluster.ip_last_observed.items()\n",
    "                    ],\n",
    "                    'Cluster_Size': len(cluster.ip_last_observed)\n",
    "                })\n",
    "                seen_cluster_ids.add(cluster.cluster_id)\n",
    "\n",
    "        latest_date = max(cluster['Last_Observed'] for cluster in all_clusters) if all_clusters else datetime.now().strftime('%Y-%m-%d')\n",
    "        for cluster in all_clusters:\n",
    "            cluster_id = cluster['ClusterID']\n",
    "            for ip_entry in cluster['Malicious_IPs']:\n",
    "                ip = ip_entry['IP']\n",
    "                if ip not in self.ip_cluster_history:\n",
    "                    self.ip_cluster_history[ip] = []\n",
    "                if ip in self.ip_cluster_tracker:\n",
    "                    last_date, last_cluster_id = self.ip_cluster_tracker[ip]\n",
    "                    if last_date == latest_date and last_cluster_id == cluster_id:\n",
    "                        continue\n",
    "                    elif last_cluster_id != cluster_id:\n",
    "                        self.ip_cluster_history[ip].append({\"ClusterID\": cluster_id, \"Date\": latest_date})\n",
    "                        self.ip_cluster_tracker[ip] = (latest_date, cluster_id)\n",
    "                else:\n",
    "                    self.ip_cluster_history[ip].append({\"ClusterID\": cluster_id, \"Date\": latest_date})\n",
    "                    self.ip_cluster_tracker[ip] = (latest_date, cluster_id)\n",
    "\n",
    "        analysis_data = [{k: v for k, v in cluster.items() if k != 'Malicious_IPs'} for cluster in all_clusters]\n",
    "\n",
    "        ip_last_cluster = {}\n",
    "        ip_first_observed = {}\n",
    "        ip_last_observed = {}\n",
    "        for cluster in all_clusters:\n",
    "            cluster_id = cluster['ClusterID']\n",
    "            for ip_entry in cluster['Malicious_IPs']:\n",
    "                ip = ip_entry['IP']\n",
    "                current_observed = ip_entry.get('Last_Observed')\n",
    "                if current_observed:\n",
    "                    if ip not in ip_first_observed or current_observed < ip_first_observed[ip]:\n",
    "                        ip_first_observed[ip] = current_observed\n",
    "                    if ip not in ip_last_observed or current_observed > ip_last_observed[ip]:\n",
    "                        ip_last_observed[ip] = current_observed\n",
    "                    if ip not in ip_last_cluster or current_observed >= ip_last_cluster[ip][1]:\n",
    "                        ip_last_cluster[ip] = (cluster_id, current_observed)\n",
    "\n",
    "        ip_data = [\n",
    "            {\n",
    "                'Malicious_IP': ip,\n",
    "                'Clusters': [cluster_id],\n",
    "                'First_Observed': ip_first_observed[ip],\n",
    "                'Last_Observed': ip_last_observed[ip],\n",
    "                'Observation_Count': self.ip_observation_counts.get(ip, 1)\n",
    "            }\n",
    "            for ip, (cluster_id, _) in ip_last_cluster.items()\n",
    "        ]\n",
    "\n",
    "        ip_history_items = [\n",
    "            {'IP': ip, 'Date': entry['Date'], 'ClusterID': entry['ClusterID']}\n",
    "            for ip, history in self.ip_cluster_history.items()\n",
    "            for entry in history\n",
    "        ]\n",
    "\n",
    "        # Clean NaN values\n",
    "        analysis_data = self._clean_nan(analysis_data)\n",
    "        ip_data = self._clean_nan(ip_data)\n",
    "        ip_history_items = self._clean_nan(ip_history_items)\n",
    "\n",
    "        # Save to JSON files\n",
    "        try:\n",
    "            with open(self.analysis_file, 'w') as f:\n",
    "                json.dump(analysis_data, f, indent=4)\n",
    "            self.logger.info(f\"Saved {len(analysis_data)} clusters to {self.analysis_file}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save to {self.analysis_file}: {e}\")\n",
    "\n",
    "        try:\n",
    "            with open(self.ip_file, 'w') as f:\n",
    "                json.dump(ip_data, f, indent=4)\n",
    "            self.logger.info(f\"Saved {len(ip_data)} IPs to {self.ip_file}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save to {self.ip_file}: {e}\")\n",
    "\n",
    "        try:\n",
    "            with open(self.ip_history_file, 'w') as f:\n",
    "                json.dump(ip_history_items, f, indent=4)\n",
    "            self.logger.info(f\"Saved {len(ip_history_items)} IP history entries to {self.ip_history_file}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save to {self.ip_history_file}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(all_clusters)\n",
    "        df['Last_Observed'] = pd.to_datetime(df['Last_Observed'], errors='coerce')\n",
    "        df['First_Observed'] = pd.to_datetime(df['First_Observed'], errors='coerce')\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = BotnetClusterProcessor(\n",
    "        merged_df,\n",
    "        analysis_file='cluster_analysis.json',\n",
    "        ip_file='cluster_malicious_ips.json',\n",
    "        ip_history_file='cluster_ip_history.json'\n",
    "    )\n",
    "    processor.process()\n",
    "    cluster_df = processor.save_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
